{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4ca6c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import numpy as np\n",
    "import faiss\n",
    "import openai\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f8717a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install -c conda-forge PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9cab7014",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_document_ids(db_path):\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT goid FROM subset_table\")  # Adjust based on your table schema\n",
    "    ids = [row[0] for row in cursor.fetchall()]\n",
    "    conn.close()\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "afbf18e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path = 'subset_data.db'\n",
    "document_ids = fetch_document_ids(db_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "50347767",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_documents_by_ids(db_path, document_ids):\n",
    "    documents = []\n",
    "    # Open a single connection\n",
    "    with sqlite3.connect(db_path) as conn:\n",
    "        for doc_id in document_ids:\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute(\"SELECT text FROM subset_table WHERE goid = ?\", (doc_id,))\n",
    "            result = cursor.fetchone()\n",
    "            if result:\n",
    "                documents.append(result[0])\n",
    "            else:\n",
    "                documents.append(None)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "3ecfcc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hnsw_faiss_index(encoded_docs, M=16, efConstruction=200):\n",
    "    \"\"\"\n",
    "    Create an HNSW FAISS index.\n",
    "\n",
    "    Parameters:\n",
    "    - encoded_docs: numpy array of vectors to be indexed.\n",
    "    - M: The number of bi-directional links created for each element in the index. Higher values lead to higher accuracy but also higher memory consumption.\n",
    "    - efConstruction: Controls the size of the dynamic list for the construction phase, affecting index construction speed and quality.\n",
    "\n",
    "    Returns:\n",
    "    - A trained FAISS HNSW index.\n",
    "    \"\"\"\n",
    "    dimension = encoded_docs.shape[1]\n",
    "    \n",
    "    # Create an HNSW index\n",
    "    index = faiss.IndexHNSWFlat(dimension, M, faiss.METRIC_L2)\n",
    "    index.hnsw.efConstruction = efConstruction\n",
    "    \n",
    "    # No need to check if the index is trained since HNSW does not require explicit training\n",
    "    index.add(encoded_docs)\n",
    "    \n",
    "    # Optionally, you can set efSearch parameter (size of the dynamic list for searching) after adding the data\n",
    "    # For example, index.hnsw.efSearch = 64\n",
    "    \n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "05ab666e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_loaded = np.load('embeddings_docs_bert_base.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "d259076a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fa_index = create_quantized_faiss_index(embeddings_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "c857e6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_quantized_faiss_index(encoded_docs):\n",
    "    dimension = encoded_docs.shape[1]\n",
    "    nlist = min(len(encoded_docs), 2048)\n",
    "    quantizer = faiss.IndexFlatL2(dimension)\n",
    "    index = faiss.IndexIVFPQ(quantizer, dimension, nlist, 256, 8) \n",
    "    index.nprobe = 20\n",
    "    assert not index.is_trained\n",
    "    index.train(encoded_docs)\n",
    "    index.add(encoded_docs)\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "a34233c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_flat_faiss_index(encoded_docs):\n",
    "    dimension = encoded_docs.shape[1]\n",
    "    # Create a flat L2 index\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    # No need to train a flat index, so we can directly add the documents\n",
    "    index.add(encoded_docs)\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "7f22c281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS generation took 0.00 seconds for 10000 samples with distilibert\n"
     ]
    }
   ],
   "source": [
    "index = create_flat_faiss_index(embeddings_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "80d8b998",
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss.write_index(index, \"saved_index_subset_flat_512.index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "6bdb808b",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_in = faiss.read_index(\"saved_index_subset_flat_512.index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "005390e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_single_embedding(text, model, tokenizer):\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=256)\n",
    "    inputs = {name: tensor.to(device) for name, tensor in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        embedding = outputs.last_hidden_state[:,0,:].cpu().numpy()\n",
    "    return embedding\n",
    "\n",
    "_vector = generate_single_embedding(text, model, tokenizer)\n",
    "faiss.normalize_L2(_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "31764620",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is World War 2?\"\n",
    "\n",
    "def search_faiss(query, faiss_index, k=5):\n",
    "    faiss_index.nprobe = 100\n",
    "    encoded_query = generate_single_embedding(query, model, tokenizer)\n",
    "    distances, indices = faiss_index.search(encoded_query, k)\n",
    "    return [i for i in indices[0] if i < len(documents)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "f0e234f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1, -1, -1, -1, -1]"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_faiss(\" Those of Lansdowne, Lawrence and Sheppard Placed in Arlington Receiving Vault.   Special to The New York Times.   WASHINGTON, Sept. 5\", fa_index )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_py",
   "language": "python",
   "name": "my_py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
