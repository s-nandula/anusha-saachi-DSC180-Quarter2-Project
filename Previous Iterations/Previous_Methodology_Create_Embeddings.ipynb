{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9aea8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import statements\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import openai\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c166029a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#get document IDs from SQLite DB\n",
    "def fetch_document_ids(db_path):\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT goid FROM subset_table\")  # Adjust based on your table schema\n",
    "    ids = [row[0] for row in cursor.fetchall()]\n",
    "    conn.close()\n",
    "    return ids\n",
    "db_path = 'subset_data.db'\n",
    "document_ids = fetch_document_ids(db_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a0bc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get from SQLite DB using the retrieved document IDs, store as an array\n",
    "def fetch_documents_by_ids(db_path, document_ids):\n",
    "    documents = []\n",
    "    # Open a single connection\n",
    "    with sqlite3.connect(db_path) as conn:\n",
    "        for doc_id in document_ids:\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute(\"SELECT text FROM subset_table WHERE goid = ?\", (doc_id,))\n",
    "            result = cursor.fetchone()\n",
    "            if result:\n",
    "                documents.append(result[0])\n",
    "            else:\n",
    "                documents.append(None)\n",
    "    return documents\n",
    "\n",
    "documents = fetch_documents_by_ids(db_path, document_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9982d843",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the DistiliBert Tokenizer and Model (Note the exact methodology was replicated for other embedding models)\n",
    "tokenizer = AutoTokenizer.from_pretrained('tokenizer')\n",
    "model = AutoModel.from_pretrained('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d4a71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the model to the GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "model = model.to(device)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f937921a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to generate embeddings via batch processing\n",
    "def generate_embeddings(texts, model, tokenizer, batch_size=256):\n",
    "    model.eval()  # Put the model in evaluation mode\n",
    "    embeddings = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        inputs = tokenizer(batch, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "        \n",
    "        # Move your inputs to the GPU\n",
    "        inputs = {name: tensor.to(device) for name, tensor in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            # Move the embeddings back to CPU for further processing if needed\n",
    "            embeddings.extend(outputs.last_hidden_state[:,0,:].cpu().numpy())\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c0f115",
   "metadata": {},
   "outputs": [],
   "source": [
    "#time the embeddings for comparison between embedding models\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# Generate embeddings using the quantized model\n",
    "embeddings = generate_embeddings(documents[0:100000], model, tokenizer)\n",
    "\n",
    "end_time = time.time()\n",
    "duration = end_time - start_time\n",
    "print(f\"Embedding generation took {duration:.2f} seconds for {100000} samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75ade0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the embeddings to be called in later\n",
    "np.save('embeddings_docs_bert_base.npy', embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1f1daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate one embedding for the query\n",
    "def generate_single_embedding(text, model, tokenizer):\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=256)\n",
    "    inputs = {name: tensor.to(device) for name, tensor in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        embedding = outputs.last_hidden_state[:,0,:].cpu().numpy()\n",
    "    return embedding"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda12.2",
   "language": "python",
   "name": "cuda12.2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
