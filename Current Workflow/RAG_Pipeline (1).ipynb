{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdd73941",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/.conda/envs/cuda12.2/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import numpy as np\n",
    "import openai\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import chromadb\n",
    "import llm\n",
    "from transformers import pipeline\n",
    "import langchain\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56d39714",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vectordb = Chroma(persist_directory=\"./chroma_db\", embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56d5053a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [01:01<00:00, 30.60s/it]\n"
     ]
    }
   ],
   "source": [
    "model_dir = 'Llama-2-7b-chat-hf/'\n",
    "model = LlamaForCausalLM.from_pretrained(model_dir, local_files_only = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "932b76a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "72c98589",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    temperature= 0.8, \n",
    "    min_new_tokens = 150\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2b1aef13",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFacePipeline(pipeline=pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "95cde071",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectordb.as_retriever()\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever, \n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2672c4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag(qa, query):\n",
    "    print(f\"Query: {query}\\n\")\n",
    "    result = qa.run(query)\n",
    "    print(\"\\nResult: \", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58af3f9c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "instruction = \"You must provide only one answer, and it must be based solely on the context provided.\"\n",
    "query = \"What year did Herbert Hoover become president\"\n",
    "rag(qa, query + instruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b32105",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_cuda12.2",
   "language": "python",
   "name": "conda_cuda12.2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
